<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Talks & Presentations</title>
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: auto; padding: 20px; }
        h1 { text-align: center; }
        .talk { border-bottom: 1px solid #ddd; padding: 15px 0; }
        .talk h2 { margin-bottom: 5px; }
        .talk .date, .talk .venue { font-style: italic; color: #555; }
        .talk a { color: #0073e6; text-decoration: none; }
    </style>
</head>
<body>

    <h1>Talks & Presentations</h1>

    <div class="talk">
        <h2>Learning like human annotators: Cyberbullying detection in lengthy social media sessions</h2>
        <p class="date">17th May, 2023</p>
        <p class="venue">Scape Canalside Room TR3, Mile End,Lonodn(Queen Mary University of London)</p>
        <p>We present the first approach to handling lengthy social media sessions for cyberbullying detection using Transformer models. While conventional models are typically limited to 512 tokens, our proposed methods enable processing sessions of unrestricted length. Through extensive experiments on two datasets with six different Transformer models, we demonstrate the effectiveness of our approach, surpassing a wide range of competitive baselines. Our findings validate the hypothesis that lengthy social media sessions can be effectively analyzed as an aggregate of smaller fragments and that cyberbullying incidents can occur at various points within a session. This work paves the way for more comprehensive and scalable cyberbullying detection systems.</p>
        <p><a href="congnitive_1705.pdf">Slides</a></p>
    </div>

    <div class="talk">
        <h2>'Bias' in Natural Language Processing</h2>
        <p class="date">24th July, 2023</p>
        <p class="venue">British Library, 96 Euston Rd., London (Alan Turing Institute)</p>
        <p>Natural Language Processing (NLP) has revolutionized human-computer interaction, enabling applications like chatbots, machine translation, and sentiment analysis. However, as NLP models become more sophisticated, concerns regarding bias in these systems have emerged. Bias in NLP can arise from multiple sources, including training data, model architectures, and human labeling practices. Such biases can lead to unfair and discriminatory outcomes, disproportionately affecting marginalized groups and reinforcing societal prejudices. This presentation explores the origins of bias in NLP, its impact on real-world applications, and strategies for mitigating it. By understanding and addressing bias, we can develop more ethical, fair, and inclusive NLP systems that serve diverse populations without perpetuating harm.</p>
        <p><a href="Turing_bias_0721.pdf">Slides</a></p>
    </div>

</body>
</html>
